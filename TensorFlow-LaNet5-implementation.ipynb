{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Convolutional Neuron Network LeNet-5 implemented by Google TensorFlow\n",
    "=============\n",
    "\n",
    "------------\n",
    "\n",
    "<a src=\"http://yann.lecun.com/exdb/lenet/index.html\">\n",
    "LeNet-5</a> is a classic CNN model which achieves 0.95% error rate on MNIST.\n",
    "\n",
    "<img src=\"http://yann.lecun.com/exdb/lenet/gifs/asamples.gif\">\n",
    "\n",
    "This TensorFlow implementation is modified from a TensorFlow <a src=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb\">example</a>. The final test accuracy is 90.4% (further turing should be able to improve this result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "## Step 2: Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Define a data flow graph to representing a TensorFlow computation\n",
    "\n",
    "The model structure of LeNet-5:\n",
    "<img src=\"https://www.researchgate.net/profile/Haohan_Wang/publication/282997080/figure/fig10/AS:305939199610894@1449952997905/Figure-10-Architecture-of-LeNet-5-one-of-the-first-initial-architectures-of-CNN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "         [batch, height, width, channel]\n",
      "data:    [16, 28, 28, 1]\n",
      "C1:      [16, 28, 28, 6]\n",
      "S2:      [16, 14, 14, 6]\n",
      "C3:      [16, 10, 10, 16]\n",
      "S4:      [16, 5, 5, 16]\n",
      "reshape: [16, 400]\n",
      "C5:      [16, 120]\n",
      "F6:      [16, 84]\n",
      "OUTPUT:  [16, 10] \n",
      "\n",
      "\n",
      "VALIDATION\n",
      "         [batch, height, width, channel]\n",
      "data:    [10000, 28, 28, 1]\n",
      "C1:      [10000, 28, 28, 6]\n",
      "S2:      [10000, 14, 14, 6]\n",
      "C3:      [10000, 10, 10, 16]\n",
      "S4:      [10000, 5, 5, 16]\n",
      "reshape: [10000, 400]\n",
      "C5:      [10000, 120]\n",
      "F6:      [10000, 84]\n",
      "OUTPUT:  [10000, 10] \n",
      "\n",
      "\n",
      "TESTING\n",
      "         [batch, height, width, channel]\n",
      "data:    [10000, 28, 28, 1]\n",
      "C1:      [10000, 28, 28, 6]\n",
      "S2:      [10000, 14, 14, 6]\n",
      "C3:      [10000, 10, 10, 16]\n",
      "S4:      [10000, 5, 5, 16]\n",
      "reshape: [10000, 400]\n",
      "C5:      [10000, 120]\n",
      "F6:      [10000, 84]\n",
      "OUTPUT:  [10000, 10] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "C1_depth = 6\n",
    "C3_depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset) ## the shape is (10000, 28, 28)\n",
    "  tf_test_dataset = tf.constant(test_dataset)   ## the shape is (10000, 28, 28)\n",
    "  \n",
    "  # Variables.\n",
    "  C1_filter = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, C1_depth], stddev=0.1))\n",
    "  C1_biases = tf.Variable(tf.zeros([C1_depth]))\n",
    "  C3_filter = tf.Variable(tf.truncated_normal([patch_size, patch_size, C1_depth, C3_depth], stddev=0.1))\n",
    "  C3_biases = tf.Variable(tf.constant(1.0, shape=[C3_depth]))\n",
    "  C5_weights = tf.Variable(tf.truncated_normal([400, 120], stddev=0.1))\n",
    "  C5_biases = tf.Variable(tf.constant(1.0, shape=[120]))\n",
    "  F6_weights = tf.Variable(tf.truncated_normal([120, 84], stddev=0.1))\n",
    "  F6_biases = tf.Variable(tf.constant(1.0, shape=[84]))\n",
    "  OUTPUT_weights = tf.Variable(tf.truncated_normal([84, num_labels], stddev=0.1))\n",
    "  OUTPUT_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    \n",
    "  # Model.\n",
    "    ## LeNet-5 : \n",
    "    ## 1@32x32  --conv 5x5-->  6@28x28  --max pool 2x2-->  6@14x14  --conv 5x5-->  16@10x10  --max pool 2x2--> 16@5x5\n",
    "    ## --full cont--> 120 --full cont--> 84 --full cont--> 10\n",
    "  def model(data):\n",
    "    print('         [batch, height, width, channel]')\n",
    "    print('data:   ', data.get_shape().as_list())\n",
    "    \n",
    "    C1 = tf.nn.conv2d(data, C1_filter, [1, 1, 1, 1], padding='SAME')  ## C1: 6@28x28\n",
    "    C1 = tf.nn.relu(C1 + C1_biases)\n",
    "    print('C1:     ', C1.get_shape().as_list())\n",
    "    \n",
    "    S2 = tf.nn.max_pool(C1, [1,2,2,1], [1,2,2,1], padding='VALID')    ## S2: 6@14x14\n",
    "    print('S2:     ', S2.get_shape().as_list())\n",
    "    \n",
    "    C3 = tf.nn.conv2d(S2, C3_filter, [1, 1, 1, 1], padding='VALID')   ## C3: 16@10x10\n",
    "    C3 = tf.nn.relu(C3 + C3_biases)\n",
    "    print('C3:     ', C3.get_shape().as_list())\n",
    "    \n",
    "    S4 = tf.nn.max_pool(C3, [1,2,2,1], [1,2,2,1], padding='VALID')    ## S4: 16@5x5\n",
    "    print('S4:     ', S4.get_shape().as_list())\n",
    "    \n",
    "    shape = S4.get_shape().as_list()\n",
    "    reshape = tf.reshape(S4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print('reshape:', reshape.get_shape().as_list())\n",
    "    \n",
    "    C5 = tf.nn.relu(tf.matmul(reshape, C5_weights) + C5_biases)\n",
    "    print('C5:     ', C5.get_shape().as_list())\n",
    "    \n",
    "    F6 = tf.nn.relu(tf.matmul(C5, F6_weights) + F6_biases)\n",
    "    print('F6:     ', F6.get_shape().as_list())\n",
    "    \n",
    "    OUTPUT = tf.matmul(F6, OUTPUT_weights) + OUTPUT_biases\n",
    "    print('OUTPUT: ', OUTPUT.get_shape().as_list(), '\\n\\n')\n",
    "    \n",
    "    return OUTPUT\n",
    "  \n",
    "  print('TRAINING')    \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)    \n",
    "  print('VALIDATION')\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  print('TESTING')\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run a TensorFlow session (`tf.session`) to train, validate and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Minibatch\t Minibatch\t Validation\n",
      "Step\t Loss\t\t Accuracy\t Accuracy\n",
      "0\t 4.374419\t 6.2%\t\t 10.0%\t\n",
      "50\t 2.229358\t 12.5%\t\t 21.1%\t\n",
      "100\t 1.112568\t 50.0%\t\t 53.2%\t\n",
      "150\t 0.665704\t 75.0%\t\t 68.5%\t\n",
      "200\t 0.652371\t 81.2%\t\t 73.8%\t\n",
      "250\t 1.317888\t 62.5%\t\t 74.5%\t\n",
      "300\t 0.861638\t 68.8%\t\t 78.5%\t\n",
      "350\t 0.964509\t 68.8%\t\t 78.8%\t\n",
      "400\t 1.044873\t 75.0%\t\t 78.5%\t\n",
      "450\t 0.537551\t 81.2%\t\t 79.9%\t\n",
      "500\t 0.424862\t 87.5%\t\t 80.5%\t\n",
      "550\t 0.235435\t 93.8%\t\t 81.0%\t\n",
      "600\t 0.509160\t 81.2%\t\t 81.3%\t\n",
      "650\t 0.416043\t 87.5%\t\t 82.3%\t\n",
      "700\t 0.589775\t 81.2%\t\t 81.6%\t\n",
      "750\t 0.274672\t 100.0%\t\t 82.5%\t\n",
      "800\t 0.554814\t 87.5%\t\t 83.0%\t\n",
      "850\t 0.633812\t 87.5%\t\t 82.7%\t\n",
      "900\t 0.525710\t 87.5%\t\t 81.9%\t\n",
      "950\t 0.155540\t 93.8%\t\t 83.7%\t\n",
      "1000\t 0.205820\t 100.0%\t\t 83.5%\t\n",
      "***Test accuracy: 90.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('\\t',     'Minibatch\\t', 'Minibatch\\t',  'Validation')\n",
    "  print('Step\\t', 'Loss\\t\\t',      'Accuracy\\t',  'Accuracy')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('%d\\t %f\\t %.1f%%\\t\\t %.1f%%\\t' % (\n",
    "            step,\n",
    "            l,\n",
    "            accuracy(predictions, batch_labels),\n",
    "            accuracy(valid_prediction.eval(), valid_labels)\n",
    "        ))\n",
    "  print('*** TEST ACCURACY: %.1f%% ***' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Appendix: Quick reference for some TensorFlow APIs\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "    ##\n",
    "    ## tf.nn.conv2d( \n",
    "    ##               input,            // [batch, in_height, in_width, in_channels] \n",
    "    ##               filter/kernel,    // [filter_height, filter_width, in_channels, out_channels]([16,16,1,16])\n",
    "    ##               strides,          // [1, stride, stride, 1]（一步的大小）\n",
    "    ##               padding           // 'VALID'(smaller output) of 'SAME'(auto zero padding!)\n",
    "    ##              ) \n",
    "    ##              => 4d-tensor       // A deeper feature map as next input\n",
    "    ##\n",
    "    ## tf.nn.max_pool(\n",
    "    ##               value,            // A 4-D Tensor with shape [batch, height, width, channels]\n",
    "    ##               ksize,            // The size of the window for each dimension of the input tensor.\n",
    "    ##               strides,          // The stride of the sliding window for each dimension of the input tensor.\n",
    "    ##               padding,          // 'VALID' or 'SAME'\n",
    "    ##              )\n",
    "    ##              => 4d-Tensor\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
